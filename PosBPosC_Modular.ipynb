{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb1a906",
   "metadata": {},
   "source": [
    "###  Pos B. LSTM RNN Model (Multi-feature OHLCV with Relative Strength Index (RSI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ff22294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install protobuf==3.20.3\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_lstm_func(ticker=\"AAPL\"):\n",
    "    # === Date Setup for 1-Year Window ===\n",
    "    end_date = datetime.today()\n",
    "    start_date = end_date - timedelta(days=365)\n",
    "    \n",
    "    # === Data Collection ===\n",
    "    df = yf.download(ticker, start=start_date.strftime(\"%Y-%m-%d\"), end=end_date.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    # === RSI Computation ===\n",
    "    def compute_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = delta.clip(lower=0)\n",
    "        loss = -delta.clip(upper=0)\n",
    "        avg_gain = gain.rolling(period).mean()\n",
    "        avg_loss = loss.rolling(period).mean()\n",
    "        rs = avg_gain / avg_loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    df[\"RSI\"] = compute_rsi(df[\"Close\"])\n",
    "    df = df.dropna()\n",
    "\n",
    "    # === Feature Scaling ===\n",
    "    features = [\"Close\", \"Open\", \"High\", \"Low\", \"Volume\", \"RSI\"]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(df[features])\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target = target_scaler.fit_transform(df[[\"Close\"]])\n",
    "\n",
    "    # === Sequence Preparation ===\n",
    "    X, y = [], []\n",
    "    seq_len = 20\n",
    "    for i in range(seq_len, len(scaled)):\n",
    "        X.append(scaled[i - seq_len:i])\n",
    "        y.append(target[i])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # === Train/Test Split ===\n",
    "    test_size = 20\n",
    "    X_train, y_train = X[:-test_size], y[:-test_size]\n",
    "\n",
    "    # === Model Definition ===\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    # === Model Training ===\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=5, verbose=0)\n",
    "\n",
    "    # === t+1 Closing Price Prediction ===\n",
    "    last_sequence = scaled[-seq_len:]\n",
    "    last_sequence = last_sequence.reshape(1, seq_len, len(features))\n",
    "    pred_scaled = model.predict(last_sequence)\n",
    "    predicted_price = target_scaler.inverse_transform(pred_scaled)[0][0]\n",
    "\n",
    "    return round(predicted_price, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c25b9",
   "metadata": {},
   "source": [
    "###  Pos C. Transformer Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b0c089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# === Transformer Model Definition ===\n",
    "class TransformerModel(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, seq_length, num_transformer_blocks, dropout_rate=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.positional_encoding = self.positional_encoding(seq_length)\n",
    "        self.transformer_blocks = [\n",
    "            self.transformer_block(embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "            for _ in range(num_transformer_blocks)\n",
    "        ]\n",
    "        self.dropout_1 = layers.Dropout(dropout_rate)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        word_emb = inputs\n",
    "        pos_encoding_tiled = tf.tile(self.positional_encoding, [tf.shape(inputs)[0], 1, 1])\n",
    "        word_emb += pos_encoding_tiled\n",
    "        x = self.dropout_1(word_emb, training=training)\n",
    "        for i in range(self.num_transformer_blocks):\n",
    "            x = self.transformer_blocks[i](x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "    def transformer_block(self, embed_dim, num_heads, ff_dim, dropout_rate):\n",
    "        inputs = layers.Input(shape=(None, embed_dim))\n",
    "        attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate)(inputs, inputs)\n",
    "        attention = layers.Dropout(dropout_rate)(attention)\n",
    "        attention = layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "        outputs = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(attention)\n",
    "        outputs = layers.Dropout(dropout_rate)(outputs)\n",
    "        outputs = layers.Conv1D(filters=embed_dim, kernel_size=1)(outputs)\n",
    "        outputs = layers.Dropout(dropout_rate)(outputs)\n",
    "        outputs = layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n",
    "        return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    def positional_encoding(self, seq_length):\n",
    "        pos = tf.cast(tf.range(seq_length)[:, tf.newaxis], dtype=tf.float32)\n",
    "        i = tf.cast(tf.range(self.embed_dim)[tf.newaxis, :], dtype=tf.float32)\n",
    "        angle_rads = pos / tf.pow(10000, 2 * (i // 2) / tf.cast(self.embed_dim, tf.float32))\n",
    "        angle_rads = tf.where(tf.math.equal(i % 2, 0), tf.sin(angle_rads), tf.cos(angle_rads))\n",
    "        return angle_rads[tf.newaxis, ...]\n",
    "\n",
    "# === Modularized Transformer Prediction Function ===\n",
    "def get_transformers_func(ticker=\"AAPL\"):\n",
    "    # === Data Collection (1-Year) ===\n",
    "    end_date = datetime.today()\n",
    "    start_date = end_date - timedelta(days=365)\n",
    "    df = yf.download(ticker, start=start_date.strftime(\"%Y-%m-%d\"), end=end_date.strftime(\"%Y-%m-%d\"))\n",
    "    stock_data = df.copy()\n",
    "\n",
    "    # === Data Preparation ===\n",
    "    closing_prices = stock_data[\"Close\"].values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_prices = scaler.fit_transform(closing_prices)\n",
    "\n",
    "    seq_length = 20\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_prices) - seq_length):\n",
    "        X.append(scaled_prices[i:i + seq_length])\n",
    "        y.append(scaled_prices[i + seq_length])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # === Model Hyperparameters ===\n",
    "    embed_dim = 32\n",
    "    num_heads = 2\n",
    "    ff_dim = 32\n",
    "    num_transformer_blocks = 2\n",
    "    dropout_rate = 0.1\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "\n",
    "    # === Model Initialisation ===\n",
    "    model = TransformerModel(embed_dim, num_heads, ff_dim, seq_length, num_transformer_blocks, dropout_rate)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(inputs, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss = loss_fn(targets, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    # === Model Training ===\n",
    "    num_batches = len(X) // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(num_batches):\n",
    "            start_idx, end_idx = i * batch_size, (i + 1) * batch_size\n",
    "            batch_X, batch_y = X[start_idx:end_idx], y[start_idx:end_idx]\n",
    "            train_step(batch_X, batch_y)\n",
    "\n",
    "    # === t+1 Closing Price Prediction ===\n",
    "    last_seq = X[-1].reshape(1, seq_length, 1)\n",
    "    pred_scaled = model(last_seq)\n",
    "    predicted_price = scaler.inverse_transform(pred_scaled.numpy().reshape(-1, 1))[0][0]\n",
    "\n",
    "    return round(predicted_price, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad2dfddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prediction_df(tickers, model_func, n_days=30):\n",
    "    final_df = pd.DataFrame(columns=tickers)\n",
    "\n",
    "    for ticker in tickers:\n",
    "        # Get 1 year of Price Data\n",
    "        end_date = datetime.today()\n",
    "        start_date = end_date - timedelta(days=365)\n",
    "        df = yf.download(ticker, start=start_date.strftime(\"%Y-%m-%d\"), end=end_date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "        # Skip if Close Price data is missing\n",
    "        if \"Close\" not in df.columns or df[\"Close\"].dropna().empty:\n",
    "            print(f\"Skipping {ticker} due to missing 'Close' prices.\")\n",
    "            continue\n",
    "\n",
    "        # Get last `n_days` actual prices\n",
    "        recent_prices = df[\"Close\"].dropna().tail(n_days).copy()\n",
    "\n",
    "        # Ensure we have enough recent prices\n",
    "        if len(recent_prices) < n_days:\n",
    "            print(f\"Skipping {ticker} due to insufficient data (<{n_days} rows).\")\n",
    "            continue\n",
    "\n",
    "        # Make sure it's a datetime index\n",
    "        recent_prices.index = pd.to_datetime(recent_prices.index)\n",
    "\n",
    "        # Predict t+1 Closing Price\n",
    "        predicted_price = model_func(ticker)\n",
    "        # print(predicted_price)\n",
    "\n",
    "        # Predict next date\n",
    "        predicted_date = recent_prices.index[-1] + timedelta(days=1)\n",
    "        # print(predicted_date)\n",
    "\n",
    "        # Append to existing series\n",
    "        recent_prices.loc[predicted_date] = float(predicted_price)\n",
    "        # print(recent_prices)\n",
    "        final_df[ticker] = recent_prices.sort_index()\n",
    "        # print(final_df)\n",
    "\n",
    "    if final_df.empty:\n",
    "        raise ValueError(\"No valid Stock data was found for any Ticker.\")\n",
    "\n",
    "    final_df.index.name = \"Date\"\n",
    "    return final_df "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
