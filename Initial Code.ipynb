{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1034cad-efbe-4b8b-875f-fe75c94a1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# %pip install protobuf==3.20.3\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcf92e2f-3af1-4cd7-b4b4-0dc9b5e696dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = 'ADBE'\n",
    "S2 = 'JPM'\n",
    "\n",
    "def download_stock_data(symbols, start_date, end_date):\n",
    "    return yf.download(symbols,  start=start_date.strftime(\"%Y-%m-%d\"), end=end_date.strftime(\"%Y-%m-%d\"))['Close']\n",
    "\n",
    "def find_cointegrated_pairs(data, symbols):\n",
    "    n = len(symbols)\n",
    "    score_matrix = np.zeros((n, n))\n",
    "    pvalue_matrix = np.ones((n, n))\n",
    "    beta_matrix = np.zeros((n, n))\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            S1 = data[symbols[i]]\n",
    "            S2 = data[symbols[j]]\n",
    "            result = coint(S1, S2)\n",
    "            model = sm.OLS(S1, sm.add_constant(S2)).fit()\n",
    "            score = result[0]\n",
    "            pvalue = result[1]\n",
    "            score_matrix[i, j] = score\n",
    "            pvalue_matrix[i, j] = pvalue\n",
    "            beta_matrix[i, j] = model.params[1]\n",
    "            if pvalue < 0.10:\n",
    "                print(symbols[i], symbols[j], i, j, pvalue)\n",
    "                pairs.append((symbols[i], symbols[j]))\n",
    "    \n",
    "    return score_matrix, pvalue_matrix, pairs, beta_matrix\n",
    "\n",
    "def get_pos(zscore, S1, S2, hedge_ratios):\n",
    "    # Initialize positions\n",
    "    entry_z, exit_z = 1.5, 0.5\n",
    "    long = zscore < -entry_z\n",
    "    short = zscore > entry_z\n",
    "    exit = abs(zscore) < exit_z\n",
    "    if long[-1]:\n",
    "        indicator = 1\n",
    "    elif short[-1]:\n",
    "        indicator = -1\n",
    "    elif exit[-1]:\n",
    "        print(\"EXIT!!!!!!!\")\n",
    "        indicator = 0\n",
    "        return None\n",
    "    else:\n",
    "        print(\"DONT DO ANYTHING FOR TODAY!!!!!!!\")\n",
    "        return None\n",
    "    print(f\"Stock {S1}: {indicator * 1} ----- Stock {S2}: {indicator * -1 * hedge_ratios[-1]}\")\n",
    "    return 1\n",
    "    \n",
    "def kalman_filter(symbols):\n",
    "    symbol_list = symbols\n",
    "    end_date = datetime.today()\n",
    "    start_date = end_date - timedelta(days=365)\n",
    "    data = download_stock_data(symbol_list, start_date, end_date)\n",
    "    # Drop missing data\n",
    "    data = data.dropna()\n",
    "    x = data[S1].values\n",
    "    y = data[S2].values\n",
    "    dates = data.index\n",
    "    # Kalman Filter for dynamic hedge ratio\n",
    "    kf = KalmanFilter(\n",
    "        transition_matrices=np.eye(2),\n",
    "        observation_matrices=np.vstack([x, np.ones_like(x)]).T[:, np.newaxis, :],\n",
    "        initial_state_mean=np.zeros(2),\n",
    "        initial_state_covariance=np.eye(2),\n",
    "        observation_covariance=1.0,\n",
    "        transition_covariance=0.01 * np.eye(2),\n",
    "        em_vars=['transition_covariance', 'observation_covariance'] \n",
    "    )\n",
    "    kf = kf.em(y, n_iter=10)\n",
    "    state_means, _ = kf.filter(y)\n",
    "    hedge_ratios = state_means[:, 0]\n",
    "    intercepts = state_means[:, 1]\n",
    "\n",
    "    # Compute spread and z-score\n",
    "    spread = y - hedge_ratios * x - intercepts\n",
    "    spread_series = pd.Series(spread, index=dates)\n",
    "    rolling_mean = spread_series.rolling(window=20).mean()\n",
    "    rolling_std = spread_series.rolling(window=20).std()\n",
    "    zscore = (spread_series - rolling_mean) / rolling_std\n",
    "\n",
    "    get_pos(zscore, S1, S2, hedge_ratios)\n",
    "\n",
    "def get_lstm_func(ticker=\"AAPL\"):\n",
    "    # === Date Setup for 1-Year Window ===\n",
    "    end_date = datetime.today()\n",
    "    start_date = end_date - timedelta(days=365)\n",
    "    \n",
    "    # === Data Collection ===\n",
    "    df = yf.download(ticker, start=start_date.strftime(\"%Y-%m-%d\"), end=end_date.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    # === RSI Computation ===\n",
    "    def compute_rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = delta.clip(lower=0)\n",
    "        loss = -delta.clip(upper=0)\n",
    "        avg_gain = gain.rolling(period).mean()\n",
    "        avg_loss = loss.rolling(period).mean()\n",
    "        rs = avg_gain / avg_loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    df[\"RSI\"] = compute_rsi(df[\"Close\"])\n",
    "    df = df.dropna()\n",
    "\n",
    "    # === Feature Scaling ===\n",
    "    features = [\"Close\", \"Open\", \"High\", \"Low\", \"Volume\", \"RSI\"]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(df[features])\n",
    "    target_scaler = MinMaxScaler()\n",
    "    target = target_scaler.fit_transform(df[[\"Close\"]])\n",
    "\n",
    "    # === Sequence Preparation ===\n",
    "    X, y = [], []\n",
    "    seq_len = 20\n",
    "    for i in range(seq_len, len(scaled)):\n",
    "        X.append(scaled[i - seq_len:i])\n",
    "        y.append(target[i])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # === Train/Test Split ===\n",
    "    test_size = 20\n",
    "    X_train, y_train = X[:-test_size], y[:-test_size]\n",
    "\n",
    "    # === Model Definition ===\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    # === Model Training ===\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=5, verbose=0)\n",
    "\n",
    "    # === t+1 Closing Price Prediction ===\n",
    "    last_sequence = scaled[-seq_len:]\n",
    "    last_sequence = last_sequence.reshape(1, seq_len, len(features))\n",
    "    pred_scaled = model.predict(last_sequence)\n",
    "    predicted_price = target_scaler.inverse_transform(pred_scaled)[0][0]\n",
    "\n",
    "    return round(predicted_price, 2)\n",
    "\n",
    "# === Transformer Model Definition ===\n",
    "class TransformerModel(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, seq_length, num_transformer_blocks, dropout_rate=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.positional_encoding = self.positional_encoding(seq_length)\n",
    "        self.transformer_blocks = [\n",
    "            self.transformer_block(embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "            for _ in range(num_transformer_blocks)\n",
    "        ]\n",
    "        self.dropout_1 = layers.Dropout(dropout_rate)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        word_emb = inputs\n",
    "        pos_encoding_tiled = tf.tile(self.positional_encoding, [tf.shape(inputs)[0], 1, 1])\n",
    "        word_emb += pos_encoding_tiled\n",
    "        x = self.dropout_1(word_emb, training=training)\n",
    "        for i in range(self.num_transformer_blocks):\n",
    "            x = self.transformer_blocks[i](x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "    def transformer_block(self, embed_dim, num_heads, ff_dim, dropout_rate):\n",
    "        inputs = layers.Input(shape=(None, embed_dim))\n",
    "        attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate)(inputs, inputs)\n",
    "        attention = layers.Dropout(dropout_rate)(attention)\n",
    "        attention = layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "        outputs = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(attention)\n",
    "        outputs = layers.Dropout(dropout_rate)(outputs)\n",
    "        outputs = layers.Conv1D(filters=embed_dim, kernel_size=1)(outputs)\n",
    "        outputs = layers.Dropout(dropout_rate)(outputs)\n",
    "        outputs = layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n",
    "        return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    def positional_encoding(self, seq_length):\n",
    "        pos = tf.cast(tf.range(seq_length)[:, tf.newaxis], dtype=tf.float32)\n",
    "        i = tf.cast(tf.range(self.embed_dim)[tf.newaxis, :], dtype=tf.float32)\n",
    "        angle_rads = pos / tf.pow(10000, 2 * (i // 2) / tf.cast(self.embed_dim, tf.float32))\n",
    "        angle_rads = tf.where(tf.math.equal(i % 2, 0), tf.sin(angle_rads), tf.cos(angle_rads))\n",
    "        return angle_rads[tf.newaxis, ...]\n",
    "\n",
    "# === Modularized Transformer Prediction Function ===\n",
    "def get_transformers_func(ticker=\"AAPL\"):\n",
    "    # === Data Collection (1-Year) ===\n",
    "    end_date = datetime.today()\n",
    "    start_date = end_date - timedelta(days=365)\n",
    "    df = yf.download(ticker, start=start_date.strftime(\"%Y-%m-%d\"), end=end_date.strftime(\"%Y-%m-%d\"))\n",
    "    stock_data = df.copy()\n",
    "\n",
    "    # === Data Preparation ===\n",
    "    closing_prices = stock_data[\"Close\"].values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_prices = scaler.fit_transform(closing_prices)\n",
    "\n",
    "    seq_length = 20\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_prices) - seq_length):\n",
    "        X.append(scaled_prices[i:i + seq_length])\n",
    "        y.append(scaled_prices[i + seq_length])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # === Model Hyperparameters ===\n",
    "    embed_dim = 32\n",
    "    num_heads = 2\n",
    "    ff_dim = 32\n",
    "    num_transformer_blocks = 2\n",
    "    dropout_rate = 0.1\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "\n",
    "    # === Model Initialisation ===\n",
    "    model = TransformerModel(embed_dim, num_heads, ff_dim, seq_length, num_transformer_blocks, dropout_rate)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(inputs, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss = loss_fn(targets, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    # === Model Training ===\n",
    "    num_batches = len(X) // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(num_batches):\n",
    "            start_idx, end_idx = i * batch_size, (i + 1) * batch_size\n",
    "            batch_X, batch_y = X[start_idx:end_idx], y[start_idx:end_idx]\n",
    "            train_step(batch_X, batch_y)\n",
    "\n",
    "    # === t+1 Closing Price Prediction ===\n",
    "    last_seq = X[-1].reshape(1, seq_length, 1)\n",
    "    pred_scaled = model(last_seq)\n",
    "    predicted_price = scaler.inverse_transform(pred_scaled.numpy().reshape(-1, 1))[0][0]\n",
    "\n",
    "    return round(predicted_price, 2) \n",
    "\n",
    "def build_prediction_df(tickers, model_func, n_days=30):\n",
    "    final_df = pd.DataFrame(columns=tickers)\n",
    "\n",
    "    for ticker in tickers:\n",
    "        # Get 1 year of Price Data\n",
    "        end_date = datetime.today()\n",
    "        start_date = end_date - timedelta(days=365)\n",
    "        df = yf.download(ticker, start=start_date.strftime(\"%Y-%m-%d\"), end=end_date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "        # Skip if Close Price data is missing\n",
    "        if \"Close\" not in df.columns or df[\"Close\"].dropna().empty:\n",
    "            print(f\"Skipping {ticker} due to missing 'Close' prices.\")\n",
    "            continue\n",
    "\n",
    "        # Get last `n_days` actual prices\n",
    "        recent_prices = df[\"Close\"].dropna().tail(n_days).copy()\n",
    "\n",
    "        # Ensure we have enough recent prices\n",
    "        if len(recent_prices) < n_days:\n",
    "            print(f\"Skipping {ticker} due to insufficient data (<{n_days} rows).\")\n",
    "            continue\n",
    "\n",
    "        # Make sure it's a datetime index\n",
    "        recent_prices.index = pd.to_datetime(recent_prices.index)\n",
    "\n",
    "        # Predict t+1 Closing Price\n",
    "        predicted_price = model_func(ticker)\n",
    "        # print(predicted_price)\n",
    "\n",
    "        # Predict next date\n",
    "        predicted_date = recent_prices.index[-1] + timedelta(days=1)\n",
    "        # print(predicted_date)\n",
    "\n",
    "        # Append to existing series\n",
    "        recent_prices.loc[predicted_date] = float(predicted_price)\n",
    "        # print(recent_prices)\n",
    "        final_df[ticker] = recent_prices.sort_index()\n",
    "        # print(final_df)\n",
    "\n",
    "    if final_df.empty:\n",
    "        raise ValueError(\"No valid Stock data was found for any Ticker.\")\n",
    "\n",
    "    final_df.index.name = \"Date\"\n",
    "    return final_df \n",
    "\n",
    "def get_pos2(data, symbols):\n",
    "    scores, pvalues, pairs, beta_matrix = find_cointegrated_pairs(data, symbols)\n",
    "    i = symbols.index(S1)\n",
    "    j = symbols.index(S2)\n",
    "    hedge_ratio = beta_matrix[j,i].round(2)\n",
    "\n",
    "    spread = data[S1] - hedge_ratio * data[S2]\n",
    "    z_scores = zscore(spread)\n",
    "    get_pos(z_scores, S1, S2, hedge_ratio)\n",
    "    \n",
    "def main(symbols): \n",
    "    print(\"----------POS A: KALMAN FILTER -----------------\")\n",
    "    kalman_filter(symbols)   \n",
    "    print(\"\\n\\n----------POS B: LSTM -----------------\")\n",
    "    data = build_prediction_df(symbols, get_lstm_func)\n",
    "    get_pos2(data, symbols)\n",
    "    print(\"\\n\\n----------POS C: TRANSFORMERS -----------------\")\n",
    "    data = build_prediction_df(symbols, get_transformers_func)\n",
    "    get_pos2(data, symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b4bf586-b140-4562-b6cf-d7b1500ba58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------POS A: KALMAN FILTER -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONT DO ANYTHING FOR TODAY!!!!!!!\n",
      "\n",
      "\n",
      "----------POS B: LSTM -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONT DO ANYTHING FOR TODAY!!!!!!!\n",
      "\n",
      "\n",
      "----------POS C: TRANSFORMERS -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONT DO ANYTHING FOR TODAY!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "symbols = ['ADBE', 'JPM']\n",
    "main(symbols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
